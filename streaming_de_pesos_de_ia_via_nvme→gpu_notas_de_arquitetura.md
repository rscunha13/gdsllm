# Streaming de Pesos de Modelos de IA via NVMe ‚Üí GPU (Linux)

## Contexto e Motiva√ß√£o
Este documento consolida uma discuss√£o t√©cnica sobre a evolu√ß√£o do **AirLLM** e a viabilidade de um **novo runtime de infer√™ncia** focado em *streaming expl√≠cito de pesos*, inspirado em conceitos como **DirectStorage (Microsoft)** e **GPUDirect Storage (NVIDIA)**.

O problema central identificado √© que, mesmo com otimiza√ß√µes atuais (AirLLM, DeepSpeed offload, etc.), o caminho:

```
NVMe ‚Üí RAM ‚Üí VRAM
```

continua sendo um gargalo estrutural. A proposta √© tratar o **NVMe como parte da hierarquia de mem√≥ria**, permitindo:

```
NVMe ‚Üí DMA ‚Üí VRAM
```

sem passagem intermedi√°ria pela RAM.

---

## Estado Atual (AirLLM)

### O que o AirLLM j√° resolve bem
- Separa√ß√£o f√≠sica dos pesos por *layer*.
- Tamanhos previs√≠veis e determin√≠sticos dos arquivos.
- Execu√ß√£o sequencial layer-by-layer.
- VRAM tratada como recurso escasso (cache tempor√°rio).

Esses pontos tornam o AirLLM **estruturalmente compat√≠vel** com GPUDirect Storage.

### Limita√ß√µes estruturais
- Forte depend√™ncia de PyTorch (`torch.load`, `state_dict`).
- Pesos sempre transitam pela RAM.
- PyTorch assume controle total do allocator CUDA.

Essas limita√ß√µes tornam dif√≠cil evoluir o AirLLM diretamente para um modelo de streaming NVMe‚ÜíVRAM real.

---

## Insight Central

> O AirLLM j√° resolve o *problema l√≥gico* do streaming de pesos.
>
> O que falta √© resolver o *problema f√≠sico* da movimenta√ß√£o de dados.

Isso sugere que **um novo projeto** faz mais sentido do que um fork profundo.

---

## Dire√ß√£o Proposta: Novo Runtime de Infer√™ncia

### Princ√≠pios Fundamentais

1. **Pesos n√£o passam pela RAM por padr√£o**  
   A RAM √© usada apenas para metadata, scheduling e controle.

2. **Peso ‚â† Tensor**  
   Pesos s√£o blobs residentes (ou n√£o). Tensores s√£o views tempor√°rias.

3. **NVMe faz parte da hierarquia de mem√≥ria**  
   VRAM √© um cache ativo; NVMe √© a base persistente.

---

## Roadmap de 3 Fases

### üü¢ Fase 1 ‚Äî MVP (Prova T√©cnica)
**Objetivo:** demonstrar que pesos podem ser carregados diretamente do NVMe para a VRAM usando GDS e consumidos pelo PyTorch.

- NVIDIA GPU + Linux
- GPUDirect Storage funcional
- 1 modelo suportado (ex: LLaMA-7B)
- Infer√™ncia batch=1
- Execu√ß√£o layer-by-layer
- Sem foco em performance m√°xima, apenas viabilidade

**Resultado esperado:**
> "Conseguimos inferir um modelo real sem que os pesos passem pela RAM."

---

### üü° Fase 2 ‚Äî Runtime de Streaming
**Objetivo:** transformar o MVP em um runtime utiliz√°vel.

- Scheduler de residency de layers
- Prefetch de layer N+1 enquanto N executa
- Double-buffering em VRAM
- Cache configur√°vel (quantos layers manter residentes)
- Formato de weights pr√≥prio (bin√°rio, alinhado, GDS-friendly)

**Resultado esperado:**
> Infer√™ncia est√°vel, previs√≠vel e com lat√™ncia controlada.

---

### üî¥ Fase 3 ‚Äî Generaliza√ß√£o e Integra√ß√£o
**Objetivo:** tornar o projeto relevante para o ecossistema.

- Suporte a m√∫ltiplos modelos
- Quantiza√ß√£o (INT8 / INT4)
- Integra√ß√£o opcional com frameworks (PyTorch frontend)
- Abstra√ß√£o de backend (GDS hoje, outros no futuro)
- Documenta√ß√£o e exemplos

**Resultado esperado:**
> Runtime de refer√™ncia para streaming de pesos em IA.

---

## MVP Detalhado ‚Äî GDS + PyTorch Bridge

### Escopo do MVP

- **N√£o** suportar treino
- **N√£o** suportar batching
- **N√£o** suportar modelos arbitr√°rios

Foco exclusivo:
> Provar o pipeline NVMe ‚Üí VRAM ‚Üí Compute

---

### Pipeline de Dados (MVP)

```
[layer_N.bin no NVMe]
        ‚Üì (GPUDirect Storage)
[CUDA buffer em VRAM]
        ‚Üì (view)
[Tensor PyTorch CUDA]
        ‚Üì
[Execu√ß√£o da layer]
```

---

### Componentes do MVP

#### 1. Formato de Weights

- Arquivos `.bin` por layer
- Layout flat (sem pickle)
- Alinhamento m√≠nimo (4KB ou maior)
- Metadata separada (`metadata.json`):
  - shape
  - dtype
  - ordem das layers

---

#### 2. M√≥dulo GDS (C++/CUDA)

Responsabilidades:
- Alocar buffer CUDA
- Ler arquivo do NVMe direto para VRAM (GDS)
- Expor ponteiro + tamanho

Interface conceitual:
```
GdsBuffer load_layer(path, size)
```

---

#### 3. PyTorch Bridge

- Criar tensor CUDA a partir de mem√≥ria externa
- Usar `from_blob` ou DLPack
- Garantir lifetime correto do buffer

Interface conceitual:
```
tensor = gds_tensor(path, shape, dtype)
```

---

#### 4. Scheduler Simples

- Executa layers em ordem fixa
- Libera buffer da layer anterior
- Opcional: prefetch s√≠ncrono da pr√≥xima layer

---

## Estrutura Inicial do Projeto

```
project/
 ‚îú‚îÄ model/
 ‚îÇ   ‚îú‚îÄ metadata.json
 ‚îÇ   ‚îú‚îÄ layer_000.bin
 ‚îÇ   ‚îî‚îÄ layer_001.bin
 ‚îÇ
 ‚îú‚îÄ runtime/
 ‚îÇ   ‚îú‚îÄ scheduler.py
 ‚îÇ   ‚îú‚îÄ residency.py
 ‚îÇ   ‚îú‚îÄ torch_bridge.py
 ‚îÇ   ‚îî‚îÄ gds_io.cu
 ‚îÇ
 ‚îî‚îÄ examples/
     ‚îî‚îÄ inference_demo.py
```

---

## Posicionamento do Projeto

### O que este projeto **n√£o** √©

- N√£o √© uma tentativa de reinventar frameworks de IA existentes.
- N√£o √© uma alternativa direta ao PyTorch, TensorFlow ou vLLM.
- N√£o √© focado inicialmente em treinamento em larga escala.

### O que este projeto **√©**

> Um runtime de infer√™ncia experimental e pragm√°tico que traz t√©cnicas maduras de datacenter (GPUDirect Storage) para o **mercado consumidor, homelab e workstations Linux**.

Ele existe para resolver um problema que frameworks generalistas ainda n√£o atacam bem:

- VRAM limitada em GPUs consumer
- NVMe extremamente r√°pido e subutilizado
- Lat√™ncia sens√≠vel em infer√™ncia local
- Streaming fino de pesos, n√£o de dados

### P√∫blico-alvo inicial

- Usu√°rios avan√ßados de Linux
- Homelabs
- Desenvolvedores que rodam LLMs localmente
- Pesquisadores interessados em runtimes de infer√™ncia
- Pessoas que hoje usam AirLLM, llama.cpp, vLLM em setups limitados por VRAM

---

## Por que isso faz sentido **agora**

Historicamente, tecnologias seguem este caminho:

```
Datacenter / HPC ‚Üí Workstation ‚Üí Consumidor
```

GPUDirect Storage j√° est√°:
- maduro
- testado em produ√ß√£o
- usado em treinamento e pipelines de dados

O que **n√£o** existe ainda √© sua aplica√ß√£o em:

- infer√™ncia interativa
- streaming de pesos
- ambientes dom√©sticos

Este projeto existe exatamente nesse intervalo.

---

## Narrativa do Projeto ("Why this exists")

> Modelos de IA est√£o crescendo mais r√°pido do que a VRAM.
>
> Enquanto isso, NVMe se tornou r√°pido o suficiente para atuar como uma extens√£o real da mem√≥ria.
>
> O software ainda n√£o acompanhou essa realidade.

Frameworks atuais assumem que:
- pesos devem caber inteiramente na VRAM
- ou, no m√°ximo, passar pela RAM

Este projeto quebra essa suposi√ß√£o.

Ele trata:
- VRAM como cache ativo
- NVMe como base da hierarquia de mem√≥ria
- pesos como recursos *residentes sob demanda*

Assim como engines gr√°ficas aprenderam a fazer streaming de texturas, este runtime faz streaming de **pesos de modelos**.

---

## Vis√£o de Longo Prazo

Se bem-sucedido, este projeto pode:

- inspirar mudan√ßas em frameworks maiores
- servir de base para pesquisa acad√™mica
- virar backend opcional para runtimes populares
- antecipar uma necessidade inevit√°vel do ecossistema de IA

> Streaming expl√≠cito de pesos n√£o √© um truque.
>
> √â uma consequ√™ncia inevit√°vel do crescimento dos modelos.

---

## Guia para IA Codificadora (Copilot / Claude / etc)

Este trecho serve como **orienta√ß√£o expl√≠cita para uma IA codificadora** entender o projeto **GdsLLM**, seus objetivos, restri√ß√µes e o que deve ser implementado. Ele pode ser usado diretamente como contexto inicial (system / project prompt).

---

## Nome do Projeto

**GdsLLM**

> MVP / Prova de Conceito de um runtime de infer√™ncia de LLMs com *streaming expl√≠cito de pesos* usando **GPUDirect Storage (GDS)** no Linux.

---

## Objetivo Central (n√£o negoci√°vel)

> **Demonstrar infer√™ncia de um LLM onde os pesos s√£o carregados diretamente do NVMe para a VRAM, sem transitar pela RAM do sistema.**

Se os pesos passarem pela RAM, o objetivo do projeto n√£o foi atendido.

---

## O Problema Que Estamos Resolvendo

Frameworks atuais assumem que:
- pesos do modelo devem residir inteiramente na VRAM, ou
- passar obrigatoriamente pela RAM antes de chegar √† GPU

Isso cria gargalos graves em:
- GPUs consumer (VRAM limitada)
- infer√™ncia local / homelab
- modelos grandes

O GdsLLM trata:
- **VRAM como cache ativo**
- **NVMe como base da hierarquia de mem√≥ria**

---

## Escopo do MVP (restri√ß√µes claras)

A IA codificadora **n√£o deve tentar generalizar demais**.

### O MVP DEVE:
- Rodar apenas em **Linux + NVIDIA GPU**
- Usar **GPUDirect Storage (cuFile / nvidia-fs)**
- Suportar **1 modelo fixo** (ex: LLaMA-7B)
- Executar infer√™ncia **batch = 1**
- Executar o modelo **layer-by-layer**
- Carregar **um layer por vez** do NVMe para a VRAM

### O MVP N√ÉO PRECISA:
- Treinar modelos
- Suportar batching
- Ser r√°pido ou otimizado
- Ter API est√°vel
- Suportar m√∫ltiplos modelos
- Funcionar sem GDS

---

## Pipeline de Dados Esperado

```
[layer_X.bin no NVMe]
        ‚Üì (GPUDirect Storage)
[CUDA buffer em VRAM]
        ‚Üì (tensor view)
[Tensor CUDA v√°lido no PyTorch]
        ‚Üì
[Execu√ß√£o da layer]
```

**Proibido:**
```
NVMe ‚Üí RAM ‚Üí VRAM
```

---

## Formato de Weights (assumido)

- Um arquivo `.bin` por layer
- Conte√∫do: pesos em layout flat (sem pickle)
- Alinhamento m√≠nimo: 4KB
- Metadata separada (`metadata.json`) contendo:
  - shape
  - dtype
  - ordem das layers

A IA **n√£o deve usar `torch.load()` para pesos**.

---

## Componentes Que Precisam Ser Implementados

### 1. M√≥dulo GDS (baixo n√≠vel)

Respons√°vel por:
- Inicializar cuFile
- Alocar buffer CUDA
- Ler arquivo do NVMe diretamente para VRAM

Interface conceitual:
```cpp
GdsBuffer load_layer(const char* path, size_t size);
```

---

### 2. Bridge PyTorch ‚Üî CUDA Memory

Respons√°vel por:
- Criar um tensor CUDA a partir de mem√≥ria externa
- Garantir lifetime correto do buffer

Interface conceitual:
```python
tensor = gds_tensor(path, shape, dtype)
```

---

### 3. Scheduler Simples

Respons√°vel por:
- Executar layers em ordem fixa
- Garantir que apenas um layer esteja residente
- Liberar o buffer anterior ap√≥s uso

Sem prefetch no MVP.

---

## Arquitetura Inicial Esperada

```
gdsllm/
 ‚îú‚îÄ model/
 ‚îÇ   ‚îú‚îÄ metadata.json
 ‚îÇ   ‚îú‚îÄ layer_000.bin
 ‚îÇ   ‚îî‚îÄ layer_001.bin
 ‚îÇ
 ‚îú‚îÄ runtime/
 ‚îÇ   ‚îú‚îÄ gds_io.cu        # cuFile + CUDA
 ‚îÇ   ‚îú‚îÄ torch_bridge.py # tensor from external memory
 ‚îÇ   ‚îú‚îÄ scheduler.py    # execu√ß√£o layer-by-layer
 ‚îÇ   ‚îî‚îÄ __init__.py
 ‚îÇ
 ‚îî‚îÄ examples/
     ‚îî‚îÄ inference_demo.py
```

---

## Crit√©rio de Sucesso do MVP

O MVP √© considerado bem-sucedido se:

- Um modelo real executa infer√™ncia corretamente
- Cada layer √© carregado diretamente do NVMe para a VRAM
- Nenhum peso passa pela RAM do sistema
- O processo √© reproduz√≠vel

Performance **n√£o** √© crit√©rio nesta fase.

---

## Mentalidade Esperada da IA Codificadora

- Priorizar **clareza arquitetural** sobre otimiza√ß√£o
- Preferir c√≥digo expl√≠cito a abstra√ß√µes m√°gicas
- Assumir que este √© um **runtime experimental**
- Tratar GDS como *first-class citizen*

> Este projeto n√£o √© um fork de AirLLM.
> 
> √â um novo runtime inspirado por suas ideias.

---

## Considera√ß√µes Finais

- O AirLLM continua sendo uma excelente refer√™ncia conceitual.
- A proposta aqui √© **um salto arquitetural**, n√£o apenas uma otimiza√ß√£o.
- O projeto come√ßa nichado (NVIDIA + Linux), mas resolve um problema inevit√°vel do futuro da IA.

> Modelos est√£o grandes demais para a VRAM.
>
> Streaming expl√≠cito de pesos n√£o √© opcional ‚Äî √© inevit√°vel.

Este documento serve como **ponto de continuidade** para discuss√µes futuras, design detalhado e implementa√ß√£o.

